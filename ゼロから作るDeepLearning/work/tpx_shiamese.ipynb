{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "provincial-walter",
   "metadata": {
    "id": "developed-playing"
   },
   "source": [
    "Siamese_tpxtech　テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "warming-adrian",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "corrected-concern",
    "outputId": "6804e4d6-7c9e-445a-dcb2-d2654f911287"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.4.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "collective-organic",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hybrid-opera",
    "outputId": "82ff69ff-2b6b-4fdd-c55b-4d5e1393bf65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-group",
   "metadata": {
    "id": "INcDAb4RNMKq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "duplicate-stockholm",
   "metadata": {
    "id": "interstate-legislature"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "def get_dataset_slice_paths(image_dir, k):\n",
    "  '''\n",
    "  画像のパスリストを返す\n",
    "  '''\n",
    "  image_file_list = os.listdir(image_dir + k)\n",
    "  image_paths = [os.path.join(image_dir+k, fname) for fname in image_file_list]\n",
    "\n",
    "  return image_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "loving-joining",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mk6By7XKfH_d",
    "outputId": "2dff04a2-1813-4d54-8439-410c023df28f"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "operational-matrix",
   "metadata": {
    "id": "seventh-equity"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class_word = ['dust', 'friction', 'hair', 'pinhole']\n",
    "train_data = []\n",
    "train_label = []\n",
    "\n",
    "for i, k in enumerate(class_word):\n",
    "    training_image_paths = get_dataset_slice_paths('./tpxtech/', k)\n",
    "    # 画像ペアを6チャンネルとして合わせる\n",
    "    cnt = 0\n",
    "    for j in range(0, len(training_image_paths), 2):\n",
    "        im = np.array(Image.open(training_image_paths[j]))\n",
    "        im2 = np.array(Image.open(training_image_paths[j + 1]))\n",
    "        im_concat = np.concatenate([im, im2], 2)\n",
    "        im_concat = im_concat.tolist()\n",
    "        train_data.append(im_concat)\n",
    "        cnt += 1\n",
    "    train_label.extend([i for m in range(cnt)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fifteen-balance",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "trying-connecticut",
    "outputId": "df3c4993-bfc5-4721-adef-135a0da413df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((952, 80, 80, 6), (952,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_data = np.array(train_data)\n",
    "# train_label = np.array(train_label)\n",
    "# train_data.shape, train_label.shape\n",
    "train_data = np.array(train_data)\n",
    "train_label = np.array(train_label)\n",
    "train_data.shape,  train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "american-attitude",
   "metadata": {
    "id": "several-campaign"
   },
   "outputs": [],
   "source": [
    "# ペアの生成\n",
    "\n",
    "def make_pairs(images, labels):\n",
    "    pairImage = []\n",
    "    pairLabel = []\n",
    "    CLASS = 4\n",
    "    label_idx = [np.where(labels == i)[0] for i in range(CLASS)]\n",
    "    \n",
    "    for idx in range(len(images)):\n",
    "        current = images[idx]\n",
    "        label = labels[idx]\n",
    "        \n",
    "        # ポジティブ\n",
    "        for i in range(2):\n",
    "          idx_posi = np.random.choice(label_idx[label])\n",
    "          img_posi = images[idx_posi]\n",
    "          pairImage.append([current, img_posi])\n",
    "          pairLabel.append([1])\n",
    "        \n",
    "        # ネガティブ\n",
    "        for j in range(2):\n",
    "          neg = np.where(labels != label)[0]\n",
    "          idx_neg = np.random.choice(neg)\n",
    "          img_neg = images[idx_neg]\n",
    "          pairImage.append([current, img_neg])\n",
    "          pairLabel.append([0])\n",
    "\n",
    "        # # 10C2のペアを作成 => 重いのでペンディング\n",
    "        # for pair_idx in range(idx+1, len(images)):\n",
    "        #   pair_img = images[pair_idx]\n",
    "        #   pair_label = labels[pair_idx]\n",
    "        #   pairImage.append([current, pair_img])\n",
    "        #   if label == pair_label:\n",
    "        #     pairLabel.append([1])\n",
    "        #   else:\n",
    "        #     pairLabel.append([0])\n",
    "\n",
    "        \n",
    "    # ランダムに並び替える\n",
    "    per = np.random.permutation(np.arange(len(images)*4))\n",
    "    pairImage = np.array(pairImage)\n",
    "    pairImage = pairImage[per]\n",
    "    \n",
    "    pairLabel = np.array(pairLabel)\n",
    "    pairLabel = pairLabel[per]\n",
    "    \n",
    "    # テスト、バリデーションに分割して返す\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    xData_train, xData_test, yData_train, yData_test = train_test_split(pairImage, pairLabel, test_size=0.2)\n",
    "    \n",
    "    return (xData_train, xData_test, yData_train, yData_test)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-nomination",
   "metadata": {
    "id": "massive-thriller"
   },
   "outputs": [],
   "source": [
    "xData_train, xData_test, yData_train, yData_test = make_pairs(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-hamilton",
   "metadata": {
    "id": "cooperative-colorado"
   },
   "outputs": [],
   "source": [
    "xData_train.shape, xData_test.shape, yData_train.shape, yData_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dietary-labor",
   "metadata": {
    "id": "different-above"
   },
   "outputs": [],
   "source": [
    "# import for model\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Lambda, Conv2D, Dense, Dropout,GlobalAveragePooling2D, MaxPool2D\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "second-train",
   "metadata": {
    "id": "killing-vintage"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 80, 80, 6)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 80, 80, 64)        1600      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 40, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 40, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 40, 40, 64)        16448     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 20, 20, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20, 20, 64)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 48)                3120      \n",
      "=================================================================\n",
      "Total params: 21,168\n",
      "Trainable params: 21,168\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "IMG_SHAPE = (80, 80, 6)\n",
    "\n",
    "def build_siamese_model(input_shape, output_dim=48):\n",
    "    inputs = Input(input_shape)\n",
    "    x = Conv2D(64, (2,2), padding='same', activation='relu')(inputs)\n",
    "    x = MaxPool2D(pool_size=(2,2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Conv2D(64, (2,2), padding='same', activation='relu')(x)\n",
    "    x = MaxPool2D(pool_size=2)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    pooled_output = GlobalAveragePooling2D()(x)\n",
    "    outputs = Dense(output_dim)(pooled_output)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "model = build_siamese_model(IMG_SHAPE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "stone-turkey",
   "metadata": {
    "id": "diagnostic-business"
   },
   "outputs": [],
   "source": [
    "\n",
    "# １つのインスタンスを共有することでパラメータを共有する\n",
    "\n",
    "\n",
    "imgA = Input(shape=IMG_SHAPE)\n",
    "imgB = Input(shape=IMG_SHAPE)\n",
    "\n",
    "feature_extractor = build_siamese_model(IMG_SHAPE)\n",
    "modelA = feature_extractor(imgA)\n",
    "modelB = feature_extractor(imgB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "democratic-sugar",
   "metadata": {
    "id": "threaded-april"
   },
   "outputs": [],
   "source": [
    "# ユークリッド距離を計算する関数\n",
    "# レイヤとして埋め込むためにkerasで関数を生成する\n",
    "\n",
    "def euclidean_distance(vectors):\n",
    "    (A, B) = vectors\n",
    "    sumSquared = K.sum(K.square(A - B), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sumSquared, K.epsilon()))\n",
    "\n",
    "\n",
    "dist = Lambda(euclidean_distance)([modelA, modelB])\n",
    "outputs = Dense(1, activation='sigmoid')(dist)\n",
    "model = Model(inputs=[imgA, imgB], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dimensional-redhead",
   "metadata": {
    "id": "happy-homeless"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 80, 80, 6)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 80, 80, 6)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Functional)            (None, 48)           21168       input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 1)            0           model_1[0][0]                    \n",
      "                                                                 model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            2           lambda[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 21,170\n",
      "Trainable params: 21,170\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "minimal-gamma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "smaller-dutch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: brew: command not found\n"
     ]
    }
   ],
   "source": [
    "! brew install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-serbia",
   "metadata": {
    "id": "plastic-rover"
   },
   "outputs": [],
   "source": [
    "def contrastive_loss(y, preds, margin=1):\n",
    "    y = tf.cast(y, preds.dtype)\n",
    "    squaredPreds = K.square(preds)\n",
    "    squaredMargin = K.square(K.maximum(margin - preds, 0))\n",
    "    loss = K.mean(y*squaredPreds + (1 - y)*squaredMargin)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-cross",
   "metadata": {
    "id": "jewish-approach"
   },
   "outputs": [],
   "source": [
    "# 0に近い方場合同一として判定\n",
    "\n",
    "def dist_accuracy(y, preds):\n",
    "    return K.mean(K.equal(y, K.cast(preds < 0.5, y.dtype)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-pizza",
   "metadata": {
    "id": "wired-connectivity"
   },
   "outputs": [],
   "source": [
    "# binary_crossentropyの方が簡単に試せる\n",
    "model.compile(\n",
    "    loss = contrastive_loss,\n",
    "    optimizer = 'adam',\n",
    "    metrics = [dist_accuracy]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-brown",
   "metadata": {
    "id": "killing-burlington"
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_training(history):\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'], label='train loss')\n",
    "    plt.plot(history.history['val_loss'], label='val loss')\n",
    "    plt.plot(history.history['dist_accuracy'], label='train acc')\n",
    "    plt.plot(history.history['val_dist_accuracy'], label='val_acc')\n",
    "    plt.legend(loc = 'lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-finance",
   "metadata": {
    "id": "secure-superintendent"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    [xData_train[:,0], xData_train[:,1]], yData_train[:],\n",
    "    validation_data=([xData_test[:,0], xData_test[:,1]], yData_test[:]),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    epochs = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-consumption",
   "metadata": {
    "id": "informative-blanket"
   },
   "outputs": [],
   "source": [
    "xData_train[:,0].shape, yData_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-calgary",
   "metadata": {
    "id": "confident-dress"
   },
   "outputs": [],
   "source": [
    "yData_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-hindu",
   "metadata": {
    "id": "nasty-circle"
   },
   "outputs": [],
   "source": [
    "plot_training(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-liabilities",
   "metadata": {
    "id": "R87dJVs9G4cc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tpx_shiamese.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
